{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee33fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cerva\\miniconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\cerva\\miniconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\cerva\\miniconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "No GPU found. Running on CPU\n",
      "Running GP implementation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:   0%|          | 0/48 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:   2%|▏         | 1/48 [00:20<15:59, 20.41s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:   4%|▍         | 2/48 [00:39<15:06, 19.72s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:   6%|▋         | 3/48 [00:59<14:56, 19.93s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:   8%|▊         | 4/48 [01:21<15:08, 20.65s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  10%|█         | 5/48 [01:43<15:01, 20.97s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 8/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  12%|█▎        | 6/48 [02:07<15:24, 22.02s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 9/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  15%|█▍        | 7/48 [02:31<15:37, 22.88s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  17%|█▋        | 8/48 [02:56<15:38, 23.47s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 11/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  19%|█▉        | 9/48 [03:24<16:12, 24.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 12/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  21%|██        | 10/48 [03:53<16:29, 26.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 13/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  23%|██▎       | 11/48 [04:21<16:31, 26.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 14/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing windows:  25%|██▌       | 12/48 [04:51<16:40, 27.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 15/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AtomicFunction.__del__ at 0x00000282BDC13910>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cerva\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 287, in __del__\n",
      "    if self._generated_graph:\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mom_trans.changepoint_detection as cpd_gp\n",
    "from mom_trans.data_prep import calc_returns\n",
    "from data.pull_data import pull_openbb_sample_data\n",
    "import time\n",
    "\n",
    "# Load sample data\n",
    "ticker = \"ES\"\n",
    "data = pull_openbb_sample_data(ticker)\n",
    "data = data.iloc[:500]\n",
    "data[\"daily_returns\"] = calc_returns(data[\"close\"])\n",
    "\n",
    "# Run both implementations with timing\n",
    "lookback_window = 21\n",
    "output_gp = f\"gp_results{lookback_window}.csv\"\n",
    "output_rupt = f\"ruptures_results{lookback_window}.csv\"\n",
    "output_bayes = f\"bayes_results{lookback_window}.csv\"\n",
    "\n",
    "# Time the GP implementation\n",
    "print(\"Running GP implementation...\")\n",
    "start_time_gp = time.time()\n",
    "cpd_gp.run_module(data, lookback_window, output_gp)\n",
    "end_time_gp = time.time()\n",
    "gp_runtime = end_time_gp - start_time_gp\n",
    "print(f\"GP Implementation runtime: {gp_runtime:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Compare results\n",
    "gp_results = pd.read_csv(output_gp)\n",
    "\n",
    "# Print timing comparison\n",
    "print(f\"GP Implementation runtime: {gp_runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf5ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def read_processed_tickers(file_path):\n",
    "    \"\"\"\n",
    "    Safely read the processed tickers file without disrupting ongoing writes.\n",
    "    Returns a set of processed ticker symbols.\n",
    "    \"\"\"\n",
    "    processed_tickers = set()\n",
    "\n",
    "    # Check if file exists first\n",
    "    if not os.path.exists(file_path):\n",
    "        return processed_tickers\n",
    "\n",
    "    try:\n",
    "        # Try a few times in case the file is being written to\n",
    "        max_attempts = 3\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    return set(line.strip() for line in f if line.strip())\n",
    "            except (IOError, PermissionError) as e:\n",
    "                # If there's a file access issue, wait briefly and retry\n",
    "                if attempt < max_attempts - 1:\n",
    "                    print(\n",
    "                        f\"Attempt {attempt+1} to read {file_path} failed: {e}. Retrying...\"\n",
    "                    )\n",
    "                    time.sleep(0.5)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Failed to read processed tickers after {max_attempts} attempts: {e}\"\n",
    "                    )\n",
    "                    raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading processed tickers file: {e}\")\n",
    "\n",
    "    return processed_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90211de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processed 2 tickers\n",
      "Processed tickers: {'6J', '6C'}\n"
     ]
    }
   ],
   "source": [
    "from settings.default import CPD_OPENBB_OUTPUT_FOLDER\n",
    "\n",
    "# Example usage:\n",
    "lbw = 126  # Your lookback window length\n",
    "progress_file = os.path.join(\n",
    "    CPD_OPENBB_OUTPUT_FOLDER(lbw), \"processed_tickers.txt\"\n",
    ")\n",
    "processed_tickers = read_processed_tickers(progress_file)\n",
    "print(f\"Currently processed {len(processed_tickers)} tickers\")\n",
    "print(f\"Processed tickers: {processed_tickers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
